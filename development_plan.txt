Development Plan: Solar-Powered LLM System with Delay-Tolerant Networking
Project Overview
This project implements a solar-powered Large Language Model (LLM) service that utilizes principles of Delay-Tolerant Networking to handle requests when energy is available. Rather than failing during low-power periods, the system queues requests and processes them when sufficient power is available, providing users with estimated completion times.
Inspired by Low-tech Magazine's solar-powered website, this system demonstrates how AI services can operate sustainably even with intermittent power sources.
Project Goals

Create a functioning LLM system powered entirely by solar energy
Implement a queuing system for processing requests when power is available
Develop a power-aware scheduler that estimates completion times
Build a static web interface that works with intermittent availability
Provide transparent power status information to users
Create a simulation environment for testing different power scenarios

System Architecture
┌───────────────────┐     ┌─────────────────┐      ┌───────────────┐
│  Web Interface    │◄────┤ Request Scheduler│◄────┤ Power Monitor │
│  - Static content │     │ - Queue manager  │     │ - TC66C       │
│  - Status display │     │ - Time estimator │     │ - Battery sim │
└────────┬──────────┘     └────────┬─────────┘     └───────┬───────┘
         │                         │                        │
         │                         ▼                        │
         │               ┌──────────────────┐               │
         └──────────────►│  LLM Processor   │◄──────────────┘
                         │  - llama.cpp     │
                         │  - Result storage│
                         └──────────────────┘
Component Breakdown
1. Power Monitoring System
The PowerMonitor class interfaces with the TC66C USB power monitor to track current power availability and make predictions about future availability based on battery levels and weather forecasts.
pythonclass PowerMonitor:
    def __init__(self, device_path='/dev/ttyUSB0'):
        self.device_path = device_path
        self.weather_service = WeatherService()
        
    def get_current_power_reading(self):
        """Read power data from TC66C device"""
        try:
            # Open serial connection to TC66C
            # Implementation will depend on TC66C communication protocol
            # Based on the provided documentation
            readings = {
                "timestamp": int(time.time()),
                "voltage": 5.12,  # example value
                "current": 0.5,   # example value in Amps
                "power": 2.56,    # example value in Watts
                "temperature": 25 # example value
            }
            return readings
        except Exception as e:
            print(f"Error reading power monitor: {e}")
            return {"timestamp": int(time.time()), "voltage": 0, "current": 0, "power": 0, "temperature": 0}
    
    def estimate_battery_level(self):
        """Estimate battery level based on voltage"""
        voltage = self.get_current_power_reading()["voltage"]
        # Simple linear mapping from voltage to percentage
        # Adjust these values based on your battery specifications
        min_voltage = 3.2  # Battery empty
        max_voltage = 4.2  # Battery full
        
        level = (voltage - min_voltage) / (max_voltage - min_voltage) * 100
        return max(0, min(100, level))  # Clamp between 0-100%
    
    def get_solar_output(self):
        """Get current solar panel output"""
        return self.get_current_power_reading()["power"]
    
    def predict_future_availability(self, hours_ahead=24):
        """Predict power availability for upcoming hours"""
        forecast = self.weather_service.get_forecast()
        predictions = []
        
        current_hour = datetime.now().hour
        battery_level = self.estimate_battery_level()
        
        for hour in range(hours_ahead):
            forecast_hour = (current_hour + hour) % 24
            solar_estimate = self.estimate_solar_output_for_hour(forecast_hour, forecast)
            
            # Simulate battery charge/discharge
            consumption_estimate = 2.0  # Base system consumption
            net_power = solar_estimate - consumption_estimate
            
            # Update simulated battery level
            battery_level += net_power * 0.2  # Simple approximation
            battery_level = max(0, min(100, battery_level))
            
            predictions.append({
                "hour": forecast_hour,
                "solar_output": solar_estimate,
                "battery_level": battery_level,
                "processing_capable": battery_level > 30 and solar_estimate > 2.0
            })
            
        return predictions
    
    def estimate_solar_output_for_hour(self, hour, forecast):
        """Estimate solar output based on time of day and weather"""
        if 6 <= hour <= 18:  # Daylight hours
            hour_factor = 1 - abs((hour - 12) / 6)  # 0 to 1, peaking at noon
            weather_condition = forecast.get(hour, "cloudy")
            
            # Weather factors
            weather_factors = {
                "sunny": 1.0,
                "partly_cloudy": 0.7,
                "cloudy": 0.4,
                "rainy": 0.2
            }
            
            weather_factor = weather_factors.get(weather_condition, 0.5)
            max_solar_output = 30  # Watts at peak
            
            return max_solar_output * hour_factor * weather_factor
        else:
            return 0  # Night time
    
    def can_process_request(self, estimated_power_requirement):
        """Determine if there's enough power to process a request"""
        current_reading = self.get_current_power_reading()
        battery_level = self.estimate_battery_level()
        
        # Simple rule: we need at least 30% battery and more solar input than the requirement
        return battery_level > 30 and current_reading["power"] > estimated_power_requirement
    
    def get_current_status(self):
        """Get complete power status information"""
        readings = self.get_current_power_reading()
        
        return {
            "battery_level": self.estimate_battery_level(),
            "solar_output": readings["power"],
            "power_consumption": readings["power"],
            "temperature": readings["temperature"],
            "timestamp": readings["timestamp"]
        }
2. Request Queue System
The RequestQueue class manages the queue of pending LLM requests, storing them until sufficient power is available for processing.
pythonclass RequestQueue:
    def __init__(self, db_path="queue.db"):
        self.db_path = db_path
        self.init_db()
        
    def init_db(self):
        """Initialize SQLite database for persistent queue storage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Create requests table if it doesn't exist
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS requests (
            id TEXT PRIMARY KEY,
            conversation_id TEXT,
            prompt TEXT,
            submitted_at TEXT,
            estimated_power REAL,
            estimated_completion TEXT,
            status TEXT,
            response TEXT
        )
        ''')
        
        conn.commit()
        conn.close()
        
    def enqueue(self, conversation_id, prompt, estimated_power, estimated_completion):
        """Add a request to the queue"""
        request_id = str(uuid.uuid4())
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        INSERT INTO requests VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            request_id,
            conversation_id,
            prompt,
            datetime.now().isoformat(),
            estimated_power,
            estimated_completion.isoformat(),
            "queued",
            None
        ))
        
        conn.commit()
        conn.close()
        
        return request_id
        
    def get_next_processable_request(self, available_power):
        """Find first request that can be processed with available power"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
        SELECT * FROM requests 
        WHERE status = 'queued' AND estimated_power <= ? 
        ORDER BY submitted_at ASC 
        LIMIT 1
        ''', (available_power,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return dict(row)
        return None
    
    def update_request_status(self, request_id, status, response=None):
        """Update the status of a request"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if response:
            cursor.execute('''
            UPDATE requests SET status = ?, response = ? WHERE id = ?
            ''', (status, response, request_id))
        else:
            cursor.execute('''
            UPDATE requests SET status = ? WHERE id = ?
            ''', (status, request_id))
        
        conn.commit()
        conn.close()
    
    def get_request(self, request_id):
        """Get a request by ID"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM requests WHERE id = ?', (request_id,))
        row = cursor.fetchone()
        
        conn.close()
        
        if row:
            return dict(row)
        return None
    
    def get_conversation_requests(self, conversation_id):
        """Get all requests for a conversation"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
        SELECT * FROM requests WHERE conversation_id = ? ORDER BY submitted_at ASC
        ''', (conversation_id,))
        
        rows = cursor.fetchall()
        conn.close()
        
        return [dict(row) for row in rows]
    
    def get_queue_length(self):
        """Get the number of queued requests"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM requests WHERE status = "queued"')
        count = cursor.fetchone()[0]
        
        conn.close()
        return count
    
    def get_queue_position(self, request_id):
        """Get position of request in the queue"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        SELECT request_id FROM (
            SELECT id as request_id, ROW_NUMBER() OVER (ORDER BY submitted_at ASC) as position 
            FROM requests 
            WHERE status = 'queued'
        ) WHERE request_id = ?
        ''', (request_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return row[0]
        return None
3. Power-Aware Scheduler
The PowerAwareScheduler class decides when to process requests based on power availability and provides time estimates to users.
pythonclass PowerAwareScheduler:
    def __init__(self, power_monitor, request_queue, llm_processor):
        self.power_monitor = power_monitor
        self.request_queue = request_queue
        self.llm_processor = llm_processor
        self.processing = False
        self.power_calibration_data = self.load_power_calibration_data()
        
    def load_power_calibration_data(self):
        """Load power calibration data from file"""
        try:
            with open("power_calibration_data.json", "r") as f:
                return json.load(f)
        except:
            # Default power estimates if no calibration data exists
            return {
                "base_power": 2.0,  # Watts
                "token_processing_power": 0.05,  # Watts per token
                "tokens_per_second": 10  # Tokens processed per second
            }
    
    def estimate_tokens(self, prompt):
        """Estimate number of tokens in prompt"""
        # Simple estimation: 4 characters per token
        return len(prompt) // 4
    
    def estimate_power_requirement(self, token_count):
        """Estimate power requirement for processing a request"""
        # Base power + additional power per token
        return self.power_calibration_data["base_power"] + \
               (token_count * self.power_calibration_data["token_processing_power"])
    
    def estimate_processing_time(self, token_count):
        """Estimate time needed to process tokens"""
        return token_count / self.power_calibration_data["tokens_per_second"]
    
    def enqueue_prompt(self, conversation_id, prompt):
        """Add prompt to queue with power estimation"""
        # Estimate tokens and power requirements
        token_count = self.estimate_tokens(prompt)
        power_needed = self.estimate_power_requirement(token_count)
        processing_time = self.estimate_processing_time(token_count)
        
        # Get power prediction for coming hours
        power_prediction = self.power_monitor.predict_future_availability(24)
        
        # Calculate estimated completion time
        now = datetime.now()
        queue_delay = timedelta(seconds=60 * self.request_queue.get_queue_length())
        
        # Find earliest time when power will be sufficient
        estimated_completion = now + queue_delay
        for i, prediction in enumerate(power_prediction):
            if prediction["processing_capable"] and power_needed <= prediction["solar_output"]:
                estimated_completion = now + timedelta(hours=i) + timedelta(seconds=processing_time)
                break
        
        # Add to queue
        request_id = self.request_queue.enqueue(
            conversation_id, 
            prompt, 
            power_needed, 
            estimated_completion
        )
        
        # Start processing loop if not already running
        if not self.processing:
            threading.Thread(target=self.process_queue_loop).start()
            
        return request_id, estimated_completion
    
    def process_queue_loop(self):
        """Main loop for processing queued requests"""
        self.processing = True
        
        while True:
            # Check current power availability
            power_status = self.power_monitor.get_current_status()
            available_power = power_status["solar_output"]
            
            if power_status["battery_level"] > 30:
                # Get next request that can be processed with available power
                next_request = self.request_queue.get_next_processable_request(available_power)
                
                if next_request:
                    # Update status to processing
                    self.request_queue.update_request_status(next_request["id"], "processing")
                    
                    # Process the request
                    try:
                        # Monitor power during processing
                        initial_power = self.power_monitor.get_current_power_reading()
                        
                        # Generate response
                        response = self.llm_processor.generate_response(next_request["prompt"])
                        
                        # Record final power usage
                        final_power = self.power_monitor.get_current_power_reading()
                        
                        # Update power calibration data
                        self.update_power_calibration(initial_power, final_power, next_request["prompt"], response)
                        
                        # Update request status to completed
                        self.request_queue.update_request_status(
                            next_request["id"], 
                            "completed", 
                            response
                        )
                        
                        # Update conversation page
                        self.update_conversation_page(next_request["conversation_id"])
                        
                    except Exception as e:
                        print(f"Error processing request: {e}")
                        self.request_queue.update_request_status(next_request["id"], "failed")
                else:
                    # No processable requests, sleep
                    time.sleep(60)
            else:
                # Battery too low, sleep
                time.sleep(60)
            
            # Check if queue is empty and no active processing
            if self.request_queue.get_queue_length() == 0:
                break
                
        self.processing = False
    
    def update_power_calibration(self, initial_power, final_power, prompt, response):
        """Update power calibration data based on actual usage"""
        # Calculate energy used
        time_elapsed = final_power["timestamp"] - initial_power["timestamp"]
        if time_elapsed == 0:
            return
            
        average_power = (final_power["power"] + initial_power["power"]) / 2
        energy_used = average_power * (time_elapsed / 3600)  # Watt-hours
        
        # Calculate tokens
        prompt_tokens = self.estimate_tokens(prompt)
        response_tokens = self.estimate_tokens(response)
        total_tokens = prompt_tokens + response_tokens
        
        # Update calibration data
        if total_tokens > 0:
            tokens_per_second = total_tokens / time_elapsed
            token_processing_power = (average_power - self.power_calibration_data["base_power"]) / total_tokens
            
            # Update with weighted average
            self.power_calibration_data["tokens_per_second"] = (
                self.power_calibration_data["tokens_per_second"] * 0.9 + tokens_per_second * 0.1
            )
            self.power_calibration_data["token_processing_power"] = (
                self.power_calibration_data["token_processing_power"] * 0.9 + token_processing_power * 0.1
            )
            
            # Save updated calibration data
            with open("power_calibration_data.json", "w") as f:
                json.dump(self.power_calibration_data, f)
    
    def update_conversation_page(self, conversation_id):
        """Update the static HTML page for a conversation"""
        # This will be handled by the ConversationManager
        pass
4. LLM Processing System
The LlamaProcessor class interfaces with llama.cpp to process prompts with power monitoring.
pythonclass LlamaProcessor:
    def __init__(self, model_path, power_monitor):
        self.model_path = model_path
        self.power_monitor = power_monitor
        
    def generate_response(self, prompt):
        """Generate response using llama.cpp with power monitoring"""
        try:
            # Configure max tokens based on available power
            max_tokens = self.determine_max_tokens()
            
            # Start llama.cpp process
            cmd = [
                "./llama.cpp/main", 
                "-m", self.model_path,
                "-p", prompt,
                "--ctx_size", "2048",
                "--temp", "0.7",
                "--n_predict", str(max_tokens)
            ]
            
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Collect output
            output = ""
            for line in process.stdout:
                output += line
                
                # Check power periodically
                current_power = self.power_monitor.get_current_power_reading()
                
                # If battery getting too low, stop generation
                if self.power_monitor.estimate_battery_level() < 20:
                    process.terminate()
                    output += "\n[Note: Response truncated due to low power]"
                    break
            
            process.wait()
            return output
            
        except Exception as e:
            return f"Error generating response: {e}"
    
    def determine_max_tokens(self):
        """Determine maximum response length based on power availability"""
        battery_level = self.power_monitor.estimate_battery_level()
        
        if battery_level > 80:
            return 2048  # Full responses when battery is high
        elif battery_level > 50:
            return 1024  # Medium responses
        else:
            return 512   # Short responses when power is low
5. Conversation Manager
The ConversationManager class creates and updates static HTML pages for each conversation.
pythonclass ConversationManager:
    def __init__(self, static_pages_dir, request_queue, power_monitor):
        self.pages_dir = static_pages_dir
        self.request_queue = request_queue
        self.power_monitor = power_monitor
        
        # Create directory if it doesn't exist
        os.makedirs(self.pages_dir, exist_ok=True)
        
    def create_new_conversation(self, initial_prompt):
        """Create a new conversation with initial prompt"""
        # Generate unique conversation ID
        conversation_id = str(uuid.uuid4())
        
        # Create directory for this conversation
        os.makedirs(f"{self.pages_dir}/{conversation_id}", exist_ok=True)
        
        # Add prompt to scheduler queue
        scheduler = PowerAwareScheduler(self.power_monitor, self.request_queue, None)
        request_id, estimated_time = scheduler.enqueue_prompt(conversation_id, initial_prompt)
        
        # Generate initial waiting page
        self.generate_waiting_page(conversation_id, initial_prompt, request_id, estimated_time)
        
        return conversation_id
    
    def generate_waiting_page(self, conversation_id, prompt, request_id, estimated_time):
        """Generate HTML page showing waiting status"""
        # Get current power status
        power_status = self.power_monitor.get_current_status()
        queue_position = self.request_queue.get_queue_position(request_id)
        
        # Create HTML with waiting status
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Solar LLM Conversation</title>
            <meta http-equiv="refresh" content="60"> <!-- Auto-refresh every minute -->
            <style>
                body {{ font-family: system-ui, sans-serif; margin: 2em; max-width: 800px; line-height: 1.6; }}
                .battery-status {{ 
                    height: 20px; 
                    background: linear-gradient(to right, #4CAF50 {power_status['battery_level']}%, #f0f0f0 0%);
                    margin-bottom: 1em;
                    border-radius: 3px;
                }}
                .prompt, .response, .status {{
                    margin: 1em 0;
                    padding: 1em;
                    border-radius: 3px;
                }}
                .prompt {{ background: #f0f0f0; }}
                .status {{ background: #fff8e1; }}
            </style>
        </head>
        <body>
            <h1>Solar LLM Conversation</h1>
            <div class="battery-status" title="Battery level: {power_status['battery_level']}%"></div>
            
            <h2>Your prompt is in queue</h2>
            <div class="prompt">
                <p><strong>You:</strong> {prompt}</p>
            </div>
            
            <div class="status">
                <p>Estimated response time: {estimated_time.strftime('%Y-%m-%d %H:%M')}</p>
                <p>Current queue position: {queue_position if queue_position else 'Unknown'}</p>
                <p>Current solar output: {power_status['solar_output']:.2f}W</p>
                <p>Battery level: {power_status['battery_level']:.1f}%</p>
            </div>
            
            <p>This page will automatically refresh. You can also bookmark it and come back later.</p>
            <p><a href="/">Return to home page</a></p>
        </body>
        </html>
        """
        
        # Write HTML to file
        with open(f"{self.pages_dir}/{conversation_id}/index.html", 'w') as f:
            f.write(html_content)
    
    def update_conversation_page(self, conversation_id):
        """Update conversation page with completed responses"""
        # Get all requests for this conversation
        requests = self.request_queue.get_conversation_requests(conversation_id)
        
        # Get current power status
        power_status = self.power_monitor.get_current_status()
        
        # Build conversation HTML
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Solar LLM Conversation</title>
            <style>
                body {{ font-family: system-ui, sans-serif; margin: 2em; max-width: 800px; line-height: 1.6; }}
                .battery-status {{ 
                    height: 20px; 
                    background: linear-gradient(to right, #4CAF50 {power_status['battery_level']}%, #f0f0f0 0%);
                    margin-bottom: 1em;
                    border-radius: 3px;
                }}
                .prompt, .response, .status {{
                    margin: 1em 0;
                    padding: 1em;
                    border-radius: 3px;
                }}
                .prompt {{ background: #f0f0f0; }}
                .response {{ background: #e1f5fe; }}
                .status {{ background: #fff8e1; }}
                form {{ margin: 2em 0; }}
                textarea {{ width: 100%; height: 100px; padding: 0.5em; font-family: inherit; }}
                button {{ padding: 0.5em 1em; background: #4CAF50; color: white; border: none; border-radius: 3px; }}
            </style>
        </head>
        <body>
            <h1>Solar LLM Conversation</h1>
            <div class="battery-status" title="Battery level: {power_status['battery_level']}%"></div>
            
            <div class="power-info">
                <p>Current solar output: {power_status['solar_output']:.2f}W</p>
                <p>Battery level: {power_status['battery_level']:.1f}%</p>
            </div>
        """
        
        # Add conversation exchanges
        for request in requests:
            html_content += f"""
            <div class="prompt">
                <p><strong>You:</strong> {request['prompt']}</p>
            </div>
            """
            
            if request['status'] == 'completed' and request['response']:
                html_content += f"""
                <div class="response">
                    <p><strong>AI:</strong> {request['response']}</p>
                </div>
                """
            elif request['status'] == 'processing':
                html_content += f"""
                <div class="status">
                    <p>Processing your request...</p>
                    <p>This page will automatically refresh when the response is ready.</p>
                </div>
                <meta http-equiv="refresh" content="30">
                """
            elif request['status'] == 'queued':
                estimated_time = datetime.fromisoformat(request['estimated_completion'])
                html_content += f"""
                <div class="status">
                    <p>Your request is queued.</p>
                    <p>Estimated response time: {estimated_time.strftime('%Y-%m-%d %H:%M')}</p>
                    <p>This page will automatically refresh when the response is ready.</p>
                </div>
                <meta http-equiv="refresh" content="60">
                """
        
        # Add form for new prompt if most recent request is completed
        if not requests or requests[-1]['status'] == 'completed':
            html_content += f"""
            <form action="/submit" method="post">
                <input type="hidden" name="conversation_id" value="{conversation_id}">
                <textarea name="prompt" placeholder="Enter your next prompt..."></textarea>
                <button type="submit">Submit</button>
            </form>
            """
        
        html_content += """
            <p><a href="/">Return to home page</a></p>
            <p><a href="/download/{conversation_id}">Download conversation</a></p>
        </body>
        </html>
        """
        
        # Write HTML to file
        with open(f"{self.pages_dir}/{conversation_id}/index.html", 'w') as f:
            f.write(html_content)
6. Solar Simulator (for Testing)
The SolarSimulator class provides a simulation environment for testing the system without real solar power.
pythonclass SolarSimulator:
    def __init__(self):
        self.battery_capacity = 10000  # mAh
        self.battery_charge = 5000     # mAh (starting at 50%)
        self.solar_panel_watts = 30    # peak watts
        self.time_factor = 3600        # simulate at 1 hour = 1 second for testing
        self.is_processing_request = False
        self.running = False
        
    def start_simulation(self):
        """Start the solar simulation thread"""
        self.running = True
        threading.Thread(target=self.simulation_loop).start()
        
    def stop_simulation(self):
        """Stop the simulation"""
        self.running = False
        
    def simulation_loop(self):
        """Main simulation loop"""
        while self.running:
            # Get simulated time of day (0-24 hours)
            current_hour = datetime.now().hour
            
            # Calculate solar output based on time of day (simple bell curve)
            if 6 <= current_hour <= 18:  # Daylight hours
                hour_factor = 1 - abs((current_hour - 12) / 6)  # 0 to 1, peaking at noon
                cloud_factor = random.uniform(0.5, 1.0)  # Random cloud cover
                current_solar_output = self.solar_panel_watts * hour_factor * cloud_factor
            else:
                current_solar_output = 0  # Night time
                
            # Calculate power consumption
            if self.is_processing_request:
                power_consumption = 5  # watts during processing
            else:
                power_consumption = 2  # watts idle
                
            # Update battery charge
            net_power = current_solar_output - power_consumption
            charge_change = (net_power / 5) * (self.time_factor / 3600)  # mAh change
            
            self.battery_charge += charge_change
            self.battery_charge = max(0, min(self.battery_capacity, self.battery_charge))
            
            # Log status
            print(f"Hour: {current_hour}, Solar: {current_solar_output:.2f}W, " +
                  f"Consumption: {power_consumption:.2f}W, " +
                  f"Battery: {self.get_battery_percentage():.1f}%")
                  
            # Sleep for simulation time step
            time.sleep(1)
            
    def get_battery_percentage(self):
        """Get current battery level as percentage"""
        return (self.battery_charge / self.battery_capacity) * 100
        
    def get_current_power_reading(self):
        """Return simulated power readings"""
        current_hour = datetime.now().hour
        
        if 6 <= current_hour <= 18:  # Daylight hours
            hour_factor = 1 - abs((current_hour - 12) / 6)
            cloud_factor = random.uniform(0.5, 1.0)
            solar_output = self.solar_panel_watts * hour_factor * cloud_factor
        else:
            solar_output = 0
            
        power_consumption = 5.0 if self.is_processing_request else 2.0
        
        return {
            "timestamp": int(time.time()),
            "voltage": 3.7 + (self.get_battery_percentage() / 100 * 0.8),  # 3.7-4.5V range
            "current": 1.0 if self.is_processing_request else 0.4,  # A
            "power": solar_output,
            "consumption": power_consumption,
            "temperature": 25,
            "battery_percentage": self.get_battery_percentage()
        }
        
    def set_processing_state(self, is_processing):
        """Set whether the system is currently processing a request"""
        self.is_processing_request = is_processing
7. Web Server
The main Flask application that ties everything together:
pythonfrom flask import Flask, request, redirect, send_file, jsonify, render_template
import json
import os
import time

app = Flask(__name__)

# Initialize components
power_monitor = PowerMonitor()
request_queue = RequestQueue()
llm_processor = LlamaProcessor("./models/llama-7b.gguf", power_monitor)
scheduler = PowerAwareScheduler(power_monitor, request_queue, llm_processor)
conversation_manager = ConversationManager("./static/conversations", request_queue, power_monitor)

@app.route('/')
def index():
    """Home page with form to start a new conversation"""
    power_status = power_monitor.get_current_status()
    
    return render_template('index.html', 
                          battery_level=power_status['battery_level'],
                          solar_output=power_status['solar_output'])

@app.route('/new', methods=['POST'])
def new_conversation():
    """Create a new conversation"""
    initial_prompt = request.form.get('prompt')
    
    if not initial_prompt:
        return redirect('/')
    
    conversation_id = conversation_manager.create_new_conversation(initial_prompt)
    
    return redirect(f'/conversation/{conversation_id}')

@app.route('/conversation/<conversation_id>')
def view_conversation(conversation_id):
    """View a conversation"""
    # Simply serve the static HTML file
    return send_file(f"./static/conversations/{conversation_id}/index.html")

@app.route('/submit', methods=['POST'])
def submit_prompt():
    """Submit a new prompt to an existing conversation"""
    conversation_id = request.form.get('conversation_id')
    prompt = request.form.get('prompt')
    
    if not conversation_id or not prompt:
        return redirect('/')
    
    # Add prompt to scheduler queue
    request_id, estimated_time = scheduler.enqueue_prompt(conversation_id, prompt)
    
    # Update conversation page
    conversation_manager.update_conversation_page(conversation_id)
    
    return redirect(f'/conversation/{conversation_id}')

@app.route('/api/status')
def system_status():
    """API endpoint for system status"""
    power_status = power_monitor.get_current_status()
    queue_length = request_queue.get_queue_length()
    
    return jsonify({
        "battery_level": power_status["battery_level"],
        "solar_output": power_status["solar_output"],
        "queue_length": queue_length,
        "timestamp": int(time.time())
    })

@app.route('/download/<conversation_id>')
def download_conversation(conversation_id):
    """Download conversation as text file"""
    requests = request_queue.get_conversation_requests(conversation_id)
    
    text_content = f"Solar LLM Conversation {conversation_id}\n"
    text_content += f"Downloaded on {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
    
    for request in requests:
        text_content += f"You: {request['prompt']}\n\n"
        if request['status'] == 'completed' and request['response']:
            text_content += f"AI: {request['response']}\n\n"
    
    response = app.response_class(
        response=text_content,
        mimetype='text/plain',
        headers={'Content-Disposition': f'attachment;filename=conversation-{conversation_id}.txt'}
    )
    return response

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)

Development Phases
Phase 1: Basic Infrastructure

Set up Raspberry Pi with Raspbian OS
Install required Python packages and llama.cpp
Implement TC66C power monitoring integration
Create basic database for request queue
Implement static web page generation

Phase 2: Core Functionality

Develop power-aware scheduler with time estimation
Implement LLM processor with llama.cpp integration
Create conversation manager for static HTML generation
Develop basic web server for handling requests
Implement solar simulator for testing

Phase 3: Testing & Refinement

Test system under various power conditions
Refine power estimation and calibration
Improve user interface and status information
Add offline functionality (conversation downloads)
Test with real solar power (weather permitting)

Phase 4: Documentation & Presentation

Create comprehensive documentation
Develop demonstration scripts
Prepare presentation materials
Analyze system performance data
Write final report

Testing Strategy

Simulated Testing

Use solar simulator to test system under various power conditions
Simulate day/night cycles and weather patterns
Verify queue behavior during power shortages


Component Testing

Test power monitoring with TC66C
Verify LLM integration with llama.cpp
Test request queue persistence


End-to-End Testing

Submit requests during power shortage and verify they process when power returns
Test time estimation accuracy
Verify static page generation and updates


Real-World Testing

Deploy system with actual solar panel when possible
Monitor performance over multiple days
Compare actual vs. predicted power availability


Implementation Challenges & Solutions
1. Power Estimation Accuracy
Challenge: Accurately predicting LLM power consumption for different prompts.
Solution: Implement an adaptive calibration system that learns from actual power measurements during request processing. The system will continuously refine its estimates based on observed power usage patterns.
2. Handling System Shutdown
Challenge: Gracefully handling unexpected power loss during processing.
Solution: Implement regular state saving during processing. When power returns, the system can check for interrupted requests and prioritize them in the queue.
3. Weather Prediction
Challenge: Accurate solar power prediction based on weather forecasts.
Solution: Start with conservative estimates and gradually refine the model. Consider integrating with a weather API for more accurate forecasts.
4. User Engagement
Challenge: Maintaining user engagement despite potentially long wait times.
Solution: Provide transparent status updates, estimated wait times, and the option to download conversations. Implement an email notification system for completed requests.
5. Scalability
Challenge: Handling multiple users with limited power resources.
Solution: Implement a fair scheduling algorithm that balances user wait times. Consider priority tiers for different types of requests.
Dependencies & Requirements

Raspberry Pi (4 recommended) with Raspbian OS
TC66C USB power monitor
llama.cpp compatible LLM model
Python 3.9+
Flask web framework
SQLite for request queue storage
Solar panel and battery (for real-world deployment)

Features Implemented
1. Request Queue System
   - Persistent SQLite database for queue storage
   - Functions for enqueueing, updating, and retrieving requests
   - Position tracking in queue for user feedback

2. Power-Aware Scheduler
   - Scheduling based on power availability predictions
   - Adaptive power calibration for learning system characteristics
   - Estimation of processing time and power requirements

3. Mock Power Monitor
   - Simulation of power conditions for testing
   - Battery level and solar output estimation
   - Time-based power prediction

4. Mock LLM Processor
   - Simulated response generation with timing
   - Power-aware response length adjustment
   - Integration with the power monitoring system

5. Conversation Manager
   - Static HTML page generation for conversations
   - Automatic page updates when responses complete
   - Status indicators for queued and processing requests

6. Web Interface
   - Flask-based interface for submitting prompts
   - Real-time status updates with auto-refresh
   - Power status visualization
   - Download functionality for conversations

7. Testing Infrastructure
   - Unit tests for core components
   - Test databases for validation

Features To Be Implemented
1. Real LLM Integration
   - Integration with llama.cpp for actual LLM responses
   - Token counting for accurate resource estimation
   - Power-aware response generation

2. Real Power Monitoring
   - Integration with TC66C USB power monitor
   - Serial communication for real-time power readings
   - Calibration with actual hardware measurements

3. Weather Integration
   - Weather API for solar prediction
   - More accurate power forecasting

4. Notification System
   - Email notifications when responses are ready
   - Optional webhook integration

5. Enhanced Power Management
   - Graceful handling of power loss during processing
   - Request prioritization based on power forecasts

6. Deployment Components
   - Integration with actual solar panel and battery
   - System optimization for Raspberry Pi
   - Hardware setup documentation

Conclusion
This development plan outlines a comprehensive approach to creating a solar-powered LLM system with delay-tolerant networking principles. By implementing power-aware scheduling and asynchronous request processing, the system can provide AI services even with intermittent power availability.
The project demonstrates how AI systems could be designed to operate in energy-constrained environments, making them more accessible and sustainable. By prioritizing transparency and user experience, we can maintain user engagement despite the inherent delays of the system.
